# -*- coding: utf-8 -*-
"""e-commerce db using python sql

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12Td94fGs7grVV-B0dArnnYQMwIIzmKlA
"""

!pip install pyspark

!pip install findspark

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import findspark
findspark.init()
findspark.find()
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
import pyspark.pandas as ps
import warnings
warnings.filterwarnings("ignore")

spark =(
            SparkSession
                .builder
                .appName("PracticeApp")
                .master("local[4]")
    .config("spark.dynamicAllocation.enabled","false")
    .config("spark.sql.adaptive.enabled","false")
                .getOrCreate())
sc= spark.sparkContext
spark

#load each of the dataset
customer = (spark.read.option("header","true").csv("olist_customers_dataset.csv"))
geolocation = (spark.read.option("header","true").csv("olist_geolocation_dataset.csv"))
order_items = (spark.read.option("header","true").csv("olist_order_items_dataset.csv"))
order_reviews = (spark.read.option("header","true").csv("olist_order_reviews_dataset.csv"))
orders = (spark.read.option("header","true").csv("olist_orders_dataset.csv"))
payments = (spark.read.option("header","true").csv("olist_order_payments_dataset.csv"))
products = (spark.read.option("header","true").csv("olist_products_dataset.csv"))
sellers = (spark.read.option("header","true").csv("olist_sellers_dataset.csv"))

#printing schema for each
print("Customers Schema")
print(customer.printSchema())
print("\n")
print("Geolocation Schema")
print(geolocation.printSchema())
print("\n")
print("Order_items Schema")
print(order_items.printSchema())
print("\n")
print("Order Reviews Schema")
print(order_reviews.printSchema())
print("\n")
print("orders schema")
print(orders.printSchema())
print("\n")
print("Payment schema")
print(payments.printSchema())
print("\n")
print("Products Schema")
print(products.printSchema())
print("\n")
print("Sellers Schema")
print(sellers.printSchema())
print("\n")

#Create a temp view for each of the dataset above

#Customer View
customer.createOrReplaceTempView("customerView")

#Geolocation View
geolocation.createOrReplaceTempView("geolocationView")

#Order Items View
order_items.createOrReplaceTempView("order_itemsView")

#Order Reviews View
order_reviews.createOrReplaceTempView("order_reviewsView")

#Orders View
orders.createOrReplaceTempView("ordersView")

#Payment View
payments.createOrReplaceTempView("paymentView")

#Products View
products.createOrReplaceTempView("productsView")

#Sellers View
sellers.createOrReplaceTempView("sellersView")

#Create queries to display 5 records from each table

#For customer
outCustomer = spark.sql("""
	SELECT * FROM customerView
""")

print("Customer Records")
customerDF = ps.DataFrame(outCustomer)
customerDF.head(5)

#For geolocation
outGeolocation = spark.sql("""
	SELECT * FROM geolocationView
""")

print("Geolocation Records")
geolocationDF = ps.DataFrame(outGeolocation)
geolocationDF.head(5)

#FOr Order_items
outOrderItems = spark.sql("""
	SELECT * FROM order_itemsView
""")

print("Order Items Records")
orderItemsDF = ps.DataFrame(outOrderItems)
orderItemsDF.head(5)

#FOr Order Reviews
outOrderReviews = spark.sql("""
	SELECT * FROM order_reviewsView
""")

print("Order Reviews Records")
orderReviewsDF = ps.DataFrame(outOrderReviews)
orderReviewsDF.head(5)

#FOr Order
outOrders = spark.sql("""
	SELECT * FROM ordersView
""")

print("Orders Records")
ordersDF = ps.DataFrame(outOrders)
ordersDF.head(5)

#FOr Payment
outPayment = spark.sql("""
	SELECT * FROM paymentView
""")

print("Payment Records")
paymentDF = ps.DataFrame(outPayment)
paymentDF.head(5)

#FOr Products
outProducts = spark.sql("""
	SELECT * FROM productsView
""").show()

#print("Prodct Records")
#productDF = ps.DataFrame(outProducts)
#productDF.head(5)

#For Sellers
outSellers = spark.sql("""
	SELECT * FROM SellersView
""")

print("Sellers Records")
sellersDF = ps.DataFrame(outSellers)
sellersDF.head(5)

# Question 1
#1) Import the dataset and do usual exploratory analysis steps like checking the structure & characteristics
#of the dataset:

#1.1) Data type of all columns in the "customers" table.
print(customer.printSchema())

#1.2) Get the time range between when the orders were placed.
timerange = spark.sql("""
	SELECT MIN(order_purchase_timestamp) as Min_Time,MAX(order_purchase_timestamp) as Max_TIme
    FROM ordersView
    GROUP BY DATE(order_purchase_timestamp)
    ORDER BY DATE(order_purchase_timestamp)
""").show()
#timerangedf = ps.DataFrame(timerange)
#timerangedf

#1.3) Count the number of Cities and States in our dataset
count = spark.sql("""
	SELECT COUNT(DISTINCT customer_city) AS City_Count,COUNT(DISTINCT customer_state) AS State_Count
    FROM customerView
""")
countDF = ps.DataFrame(count)
print("Count of city and states in customer dataset is")
countDF

count2 = spark.sql("""
	SELECT COUNT(DISTINCT geolocation_city) AS City_Count,COUNT(DISTINCT geolocation_state) AS State_Count
    FROM geolocationView
""")
countDF2 = ps.DataFrame(count2)
print("Count of city and states in Geolocation dataset is")
countDF2

count3 = spark.sql("""
	SELECT COUNT(DISTINCT seller_city) AS City_Count,COUNT(DISTINCT seller_state) AS State_Count
    FROM sellersView
""")
countDF3 = ps.DataFrame(count3)
print("Count of city and states in Seller dataset is")
countDF3

#2. In-depth Exploration:
#1. Is there a growing trend in the no. of orders placed over the past years?

growingTrend = spark.sql(
    """
    	SELECT YEAR(o.order_purchase_timestamp) as year_placed,MONTH(o.order_purchase_timestamp) as
        month_placed,COUNT(o.order_id) as count_of_orders_placed
		FROM ordersView o
		INNER JOIN customerView c
		ON o.customer_id = c.customer_id
		GROUP BY year_placed,month_placed
		ORDER BY year_placed ASC
    """
)

growingTrendDF = ps.DataFrame(growingTrend)
growingTrendDF.head(10)

#Solution :
#Based on the results obtained it can be seen the there is an increase in the number of orders placed from 4 to 324
# and then dropped to a minimal of 1 in december
#FRom 2017 the number of orders shows a general increasing trend from January to November, with slight
#fluctuations in between.
#IN 2018 the number of orders starts high in January and remains relatively stable until May. After May,
#there is a decreasing trend in order volume.

plt.figure(figsize=(12, 6))
plt.bar(range(len(growingTrendDF)), growingTrendDF['count_of_orders_placed'].to_numpy())

for i, count in enumerate(growingTrendDF['count_of_orders_placed'].to_numpy()):
    plt.text(i, count + 0.5, str(count), ha='center')

plt.xticks(range(len(growingTrendDF)), growingTrendDF['year_placed'].astype(str).to_numpy() + '-' + growingTrendDF['month_placed'].astype(str).to_numpy())

plt.xlabel('Year-Month')
plt.ylabel('Count of Orders Placed')
plt.title('Order Placement Trend over Time')

plt.xticks(rotation=45)

plt.show()

plt.figure(figsize=(12, 6))
plt.plot(range(len(growingTrendDF)), growingTrendDF['count_of_orders_placed'].to_numpy())

for i, count in enumerate(growingTrendDF['count_of_orders_placed'].to_numpy()):
    plt.text(i, count + 0.5, str(count), ha='center')

plt.xticks(range(len(growingTrendDF)), growingTrendDF['year_placed'].astype(str).to_numpy() + '-' + growingTrendDF['month_placed'].astype(str).to_numpy())

plt.xlabel('Year-Month')
plt.ylabel('Count of Orders Placed')
plt.title('Order Placement Trend over Time')

plt.xticks(rotation=45)

plt.show()

"""2. Can we see some kind of monthly seasonality in terms of the no. of orders being placed?
SOln) monthly seasonality refers to the regular and predictable fluctuations in certain variables or phenomena that can be observed
from one month to another. From the above output
In september the no of orders placed in september 2017 was more when compared to 2016 and 2018
In october the no of orders placed in october 2017 was more when compared to 2016 and 2018
In november it was only 2017
In december 2017 had the highest no of orders placed
"""

#3) During what time of the day, do the Brazilian customers mostly place their orders? (Dawn, Morning, Afternoon or Night)
#▪ 0-6 hrs : Dawn
#▪ 7-12 hrs : Mornings
#▪ 13-18 hrs : Afternoon
#▪ 19-23 hrs : Night

brazil = spark.sql(
    """
    SELECT
        (CASE WHEN HOUR(o.order_purchase_timestamp) BETWEEN 0 AND 6 THEN 'Dawn'
              WHEN HOUR(o.order_purchase_timestamp) BETWEEN 7 AND 12 THEN 'Morning'
              WHEN HOUR(o.order_purchase_timestamp) BETWEEN 13 AND 18 THEN 'Afternoon'
              WHEN HOUR(o.order_purchase_timestamp) BETWEEN 19 AND 23 THEN 'Night'
        END) AS time_slot,
        COUNT(*) AS order_count
    FROM ordersView o
    INNER JOIN customerView c
    ON o.customer_id = c.customer_id
    GROUP BY time_slot
    ORDER BY order_count DESC
    """
)

brazilDF = ps.DataFrame(brazil)
brazilDF

"""SOlution
The highest number of orders placed by the brazilian customers is during Afternoon having order count of 38135
"""

time_slots = brazilDF['time_slot'].to_numpy()
order_counts = brazilDF['order_count'].to_numpy()

#colors = ['blue', 'green', 'purple', 'orange', 'purple']
color_map = plt.cm.get_cmap('viridis', len(time_slots))


plt.figure(figsize=(8, 6))
plt.bar(time_slots, order_counts,color = color_map(np.arange(len(time_slots))))

for i, count in enumerate(brazilDF['order_count'].to_numpy()):
    plt.text(i, count + 0.5, str(count), ha='center')
plt.xlabel('Time Slot')
plt.ylabel('Order Count')
plt.title('Order Counts by Time Slot')
plt.grid(False)

plt.show()

#3)Evolution of E-commerce orders in the Brazil region:
#1) Get the month on month no. of orders placed in each state.

monthonmonth = spark.sql("""
    WITH cte AS (
        SELECT month(CAST(o.order_purchase_timestamp AS date)) AS purchase_month,
        c.customer_state,
        COUNT(o.order_id) AS no_of_orders,
        LAG(month(CAST(o.order_purchase_timestamp AS date)), 1) OVER(PARTITION BY c.customer_state
        ORDER BY COUNT(o.order_id) DESC) AS lagged_month
        FROM ordersView o
        LEFT JOIN customerView c ON o.customer_id = c.customer_id
        GROUP BY month(CAST(o.order_purchase_timestamp AS date)), c.customer_state
    )
    SELECT purchase_month,customer_state,no_of_orders FROM cte
    ORDER BY purchase_month ASC
""").show()


#monthonmonthDF = ps.DataFrame(monthonmonth)
#monthonmonthDF.head()

#2)How are the customers distributed across all the states?
customercount = spark.sql(
    """
    SELECT customer_state,COUNT(customer_id) as count_of_customers
    FROM customerView
    GROUP BY customer_state
    Order BY count_of_customers DESC
    """
)
customercountDF = ps.DataFrame(customercount)
customercountDF.head(50)

import seaborn as sns
states = customercountDF['customer_state'].to_numpy()
customer_counts = customercountDF['count_of_customers'].to_numpy()

plt.figure(figsize=(12, 6))
plt.bar(states, customer_counts)

for i, count in enumerate(customercountDF['count_of_customers'].to_numpy()):
    plt.text(i, count + 0.5, str(count), ha='center')
plt.xlabel('States')
plt.ylabel('Customer Count')
plt.title('Distribution of Customers across States')
plt.xticks(rotation=90)
plt.grid(True)
plt.show()

#4) Impact on Economy: Analyze the money movement by e-commerce by looking at order prices, freight and others.
#4.1) Get the % increase in the cost of orders from year 2017 to 2018 (include months between Jan to Aug only).
#You can use the "payment_value" column in the payments table to get the cost of orders

percentIncrease = spark.sql("""
SELECT ((SUM(CASE WHEN YEAR(o.order_purchase_timestamp) = 2018 THEN p.payment_value ELSE 0 END)
    - SUM(CASE WHEN YEAR(o.order_purchase_timestamp) = 2017 THEN p.payment_value ELSE 0 END))
    / SUM(CASE WHEN YEAR(o.order_purchase_timestamp) = 2017 THEN p.payment_value ELSE 0 END)) * 100
    AS cost_increase_percentage
FROM ordersView o
INNER JOIN paymentView p ON
o.order_id = p.order_id
WHERE YEAR(o.order_purchase_timestamp) IN (2017,2018)
AND MONTH(o.order_purchase_timestamp) BETWEEN 1 AND 8
""")

percentIncreaseDF = ps.DataFrame(percentIncrease)
percentIncreaseDF

#4.2) Calculate the Total & Average value of order price for each state.
data = spark.sql(
    """
    	SELECT SUM(o.price) as Total_Price,AVG(o.price) as Average_Price
        FROM order_itemsView o
        INNER JOIN sellersView s
        ON o.seller_id = s.seller_id
        GROUP BY s.seller_state
    """
)

dataDF = ps.DataFrame(data)
dataDF.head(10)

#4.2) Calculate the Total & Average value of order freight for each state.
data1 = spark.sql(
    """
    	SELECT SUM(o.freight_value) as Total_Price,AVG(o.freight_value) as Average_Price
        FROM order_itemsView o
        INNER JOIN sellersView s
        ON o.seller_id = s.seller_id
        GROUP BY s.seller_state
    """
)

dataDF1 = ps.DataFrame(data1)
dataDF1.head(10)

#5)Analysis based on sales, freight and delivery time.
#5.1)Find the no. of days taken to deliver each order from the order’s purchase date as delivery time.
#Also, calculate the difference (in days) between the estimated & actual delivery date of an order.
#Do this in a single query.
noofDays = spark.sql(
    """
    SELECT order_id,(DATE(order_delivered_customer_date) - DATE(order_purchase_timestamp) ) as no_of_days,
    DATE(order_estimated_delivery_date) - DATE(order_delivered_customer_date) as estimated_days
    FROM ordersView
    """
).show()

#noofDaysDF = ps.DataFrame(noofDays)
#noofDaysDF

#5.2)Find out the top 5 states with the highest & lowest average freight value.
top5_states = spark.sql(
    """
    WITH state_avg_freight AS (
        SELECT c.customer_state, AVG(oi.freight_value) AS avg_freight
        FROM order_itemsView oi
        INNER JOIN ordersView o ON oi.order_id = o.order_id
        INNER JOIN customerView c ON c.customer_id = o.customer_id
        GROUP BY c.customer_state
    )
    SELECT customer_state, avg_freight
    FROM state_avg_freight
    ORDER BY avg_freight DESC
    LIMIT 5
    """
)
top5DF = ps.DataFrame(top5_states)
top5DF

bottom5_states = spark.sql(
    """
    WITH state_avg_freight AS (
        SELECT c.customer_state, AVG(oi.freight_value) AS avg_freight
        FROM order_itemsView oi
        INNER JOIN ordersView o ON oi.order_id = o.order_id
        INNER JOIN customerView c ON c.customer_id = o.customer_id
        GROUP BY c.customer_state
    )
    SELECT customer_state, avg_freight
    FROM state_avg_freight
    ORDER BY avg_freight ASC
    LIMIT 5
    """
)
bottom5DF = ps.DataFrame(bottom5_states)
bottom5DF.head()

#Find out the top 5 states with the highest & lowest average delivery time.
top5avg = spark.sql(
    """
    WITH top5avg AS (
        SELECT c.customer_state, AVG(CAST(o.order_estimated_delivery_date AS timestamp)) AS avg_delivery_date
        FROM customerView c
        INNER JOIN ordersView o ON c.customer_id = o.customer_id
        GROUP BY c.customer_state
    )
    SELECT * FROM top5avg
    ORDER BY avg_delivery_date DESC
    LIMIT 5
    """
)
top5avg.show()

bottom5_states = spark.sql(
    """
    WITH state_avg_freight AS (
        SELECT c.customer_state, AVG(oi.freight_value) AS avg_freight
        FROM order_itemsView oi
        INNER JOIN ordersView o ON oi.order_id = o.order_id
        INNER JOIN customerView c ON c.customer_id = o.customer_id
        GROUP BY c.customer_state
    )
    SELECT customer_state, avg_freight
    FROM state_avg_freight
    ORDER BY avg_freight ASC
    LIMIT 5
    """
)
bottom5DF = ps.DataFrame(bottom5_states)
bottom5DF.head()

#Find out the top 5 states with the highest & lowest average delivery time.
top5avg = spark.sql(
    """
    WITH top5avg AS (
        SELECT c.customer_state, AVG(CAST(o.order_estimated_delivery_date AS timestamp)) AS avg_delivery_date
        FROM customerView c
        INNER JOIN ordersView o ON c.customer_id = o.customer_id
        GROUP BY c.customer_state
    )
    SELECT * FROM top5avg
    ORDER BY avg_delivery_date DESC
    LIMIT 5
    """
)
top5avg.show()

bottom5avg = spark.sql(
    """
    WITH top5avg AS (
        SELECT c.customer_state, AVG(CAST(o.order_estimated_delivery_date AS timestamp)) AS avg_delivery_date
        FROM customerView c
        INNER JOIN ordersView o ON c.customer_id = o.customer_id
        GROUP BY c.customer_state
    )
    SELECT * FROM top5avg
    ORDER BY avg_delivery_date ASC
    LIMIT 5
    """
)
bottom5avg.show()

#6) Analysis based on the payments:
#6.1) Find the month on month no. of orders placed using different payment types.
monthOnMonth = spark.sql(
    """

WITH cte AS (
        SELECT month(CAST(o.order_purchase_timestamp AS date)) AS purchase_month, p.payment_type,
        COUNT(o.order_id) AS no_of_orders, LAG(month(CAST(o.order_purchase_timestamp AS date)), 1) OVER(PARTITION
        BY p.payment_type ORDER BY COUNT(o.order_id) DESC) AS lagged_month
        FROM ordersView o
        INNER JOIN paymentView p
        ON o.order_id = p.order_id
        GROUP BY month(CAST(o.order_purchase_timestamp AS date)), p.payment_type
    )
    SELECT purchase_month,payment_type,no_of_orders FROM cte
    order by payment_type ASC,no_of_orders ASC
    """
)

monthOnMonthDF = ps.DataFrame(monthOnMonth)
monthOnMonthDF

#6.2) Find the no. of orders placed on the basis of the payment installments that have been paid.
paymentInstallments = spark.sql(
    """
    SELECT p.payment_installments,COUNT(o.order_id) as no_of_orders
		FROM ordersView o
		INNER JOIN paymentView p
		ON o.order_id = p.order_id
    GROUP BY p.payment_installments
		ORDER BY p.payment_installments
    """
)
paymentInstallmentsDF = ps.DataFrame(paymentInstallments)
paymentInstallmentsDF.head(30)

#Misecellanous
#1)To find the count the number of orders which were paid using different methods

cards = spark.sql(
    """
    SELECT p.payment_type as payment_type,COUNT(p.order_id) as count
		FROM paymentView p
		INNER JOIN ordersView o
		ON p.order_id = o.order_id
		GROUP BY p.payment_type
    ORDER BY count DESC
    """
)
cardsDF = ps.DataFrame(cards)
cardsDF

color = ["blue","pink","green","violet","gray"]
Card_Type = cardsDF['payment_type'].to_numpy()
count = cardsDF['count'].to_numpy()
plt.figure(figsize=(8,7))
plt.bar(Card_Type, count,color = color)
for i, count in enumerate(cardsDF['count'].to_numpy()):
    plt.text(i, count + 0.5, str(count), ha='center')
plt.xlabel('Card_Type')
plt.ylabel('count')
plt.title('Payment using different methods')
plt.xticks(rotation=90)
plt.grid(False)
plt.show()

#2) Count of customers whose orders are delivered,shipped etc
status = spark.sql(
    """
    SELECT o.order_status,COUNT(c.customer_id) as count_of_customers
	FROM ordersView o
	INNER JOIN customerView c
	ON o.customer_id = c.customer_id
	GROUP BY o.order_status
    ORDER BY count_of_customers DESC
    """
)
statusDF = ps.DataFrame(status)
statusDF

#4) TO find the count of seller cities in the seller states
seller_city_count = spark.sql(
    """
    SELECT seller_state,COUNT(seller_city) as count_city
	FROM SellersView
	GROUP BY seller_state
	ORDER BY count_city DESC
    """
)
seller_city_countDF = ps.DataFrame(seller_city_count)
seller_city_countDF

plt.figure(figsize=(12, 6))
plt.bar(seller_city_countDF['seller_state'].to_numpy(), seller_city_countDF['count_city'].to_numpy())

for i, count in enumerate(seller_city_countDF['count_city'].to_numpy()):
    plt.text(i, count + 0.5, str(count), ha='center')

plt.xlabel('Seller State')
plt.ylabel('City Count')
plt.title('Number of Cities per Seller State')

plt.xticks(rotation=90)
plt.show()

#5) THe count of order ids which got highest review score
score = spark.sql(
    """
    SELECT review_score,COUNT(o.order_id) as count_of_orders
	FROM order_reviewsView r
	INNER JOIN ordersView o
	ON r.order_id = o.order_id
    GROUP BY review_score
    ORDER BY r.review_score ASC
    """
)
scoreDF = ps.DataFrame(score)
scoreDF

plt.figure(figsize=(10, 6))
line_color = "green"
plt.plot(scoreDF['review_score'].to_numpy(), scoreDF['count_of_orders'].to_numpy(), marker='o', linestyle='-',color = line_color)

for x, y in zip(scoreDF['review_score'].to_numpy(), scoreDF['count_of_orders'].to_numpy()):
    plt.text(x, y, str(y), ha='center', va='bottom')

plt.xlabel('Review Score')
plt.ylabel('Count of Orders')
plt.title('Count of Orders for Each Review Score')

# Display the plot
plt.show()

#6) Products which have the highest volume
volume = spark.sql("""
	WITH CTE AS
	(
		SELECT product_id,product_length_cm,product_width_cm,product_height_cm,
		product_length_cm*product_width_cm*product_height_cm  as volume
        FROM productsView
	)

	SELECT product_id,volume
	FROM CTE
    ORDER BY volume DESC
""")
volumeDF = ps.DataFrame(volume)
volumeDF

#PRinting the maximum and minimum value of volume
max_volume_product = volumeDF.loc[volumeDF['volume'].idxmax()]
min_volume_product = volumeDF.loc[volumeDF['volume'].idxmin()]

print("The product with id", max_volume_product['product_id'], "has the maximum volume of", max_volume_product['volume'])
print("The product with id", min_volume_product['product_id'], "has the minimum volume of", min_volume_product['volume'])

#8)To find which payment type had the highest installment
installment = spark.sql("""
	SELECT payment_type,MAX(payment_installments) as count
	FROM paymentView
	GROUP BY payment_type
    ORDER BY count DESC
""")
installmentDF = ps.DataFrame(installment)
installmentDF

#9)Count of people who gave reviews
review = spark.sql("""SELECT * FROM order_reviewsView""")

review_countDF = review.toPandas()

extractDF = review_countDF["review_comment_title"].value_counts()
solnDF = pd.DataFrame(extractDF)

solnDF.head(40)

#10) To print all schemas in one go

dataFrames = {
	'customer':customer,
	'geolocation':geolocation,
	'order_items':order_items,
	'order_reviews':order_reviews,
	'orders':orders,
	'payments':payments,
	'products':products,
	'sellers':sellers
}
for table,dataframe in dataFrames.items():
    print(f"Schema for {table}:")
    dataframe.printSchema()
    print()